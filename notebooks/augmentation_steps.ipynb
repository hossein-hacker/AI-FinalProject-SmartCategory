{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f01e71",
   "metadata": {},
   "source": [
    "# Data Augmentation Strategy & Visualization\n",
    "\n",
    "In this notebook, we define and test the visual transformation pipeline for our product dataset. Since our images are captured against a **pure white background**, our augmentation strategy focuses on:\n",
    "\n",
    "1. **Reducing Background Dependency:** Ensuring the model learns product features rather than just the object-to-white contrast.\n",
    "    \n",
    "2. **Environmental Simulation:** Using color and brightness jitters to simulate different lighting conditions.\n",
    "    \n",
    "3. **Spatial Invariance:** Using rotations and translations so the model can recognize products regardless of their position in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad19401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "\n",
    "df = pd.read_csv('../data/processed/products_cleaned.csv')\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Downloads an image from a URL and returns a PIL Image object.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} products available for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e2246",
   "metadata": {},
   "source": [
    "## Defining the Augmentation Pipeline\n",
    "\n",
    "We use `torchvision.transforms` to build the pipeline.\n",
    "\n",
    "**Crucial Setting:** Since our background is white, we use `fill=255` (or `fill=1.0` for tensors) during rotations and translations. This ensures that any \"empty space\" created by moving the image is filled with white, matching our dataset's aesthetic and preventing the model from learning black borders as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1729eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    # Stretching the image to exactly 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "    \n",
    "    # Random Rotation: Up to 20 degrees, filled with white (to match background)\n",
    "    transforms.RandomRotation(degrees=20, fill=255),\n",
    "    \n",
    "    # Random Translation: Shifting the product slightly\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=255),\n",
    "    \n",
    "    # Color/Light Jitter: Simulating different room lightings\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
    "    \n",
    "    # Horizontal Flip\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Convert to Tensor and Normalize\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Augmentation pipeline with 224x224 Stretch Resize is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be5701",
   "metadata": {},
   "source": [
    "## Why Stretch Resize?\n",
    "\n",
    "By using `transforms.Resize((224, 224))`, we ensure every input image has the exact same dimensions required by architectures like **ResNet** or **EfficientNet**.\n",
    "\n",
    "- **Pros:** Every pixel in the 224x224 grid contains data; no \"wasted\" space with black or white bars.\n",
    "    \n",
    "- **Cons:** If an image is very wide (like a belt) or very tall (like a floor lamp), stretching might slightly distort the product's shape. However, CNNs are generally robust enough to handle this minor distortion during classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = df.sample(1).iloc[0]\n",
    "print(f\"Visualizing: {sample_row['title']}\")\n",
    "original_img = load_image_from_url(sample_row['imgUrl'])\n",
    "\n",
    "if original_img:\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original (Raw)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Apply Augmentations 7 times\n",
    "    for i in range(2, 9):\n",
    "        viz_transforms = transforms.Compose(train_transforms.transforms[:-1]) \n",
    "        \n",
    "        augmented_tensor = viz_transforms(original_img)\n",
    "        # Convert Tensor (C,H,W) to Numpy (H,W,C) for Matplotlib\n",
    "        augmented_img = augmented_tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.subplot(2, 4, i)\n",
    "        plt.imshow(augmented_img)\n",
    "        plt.title(f\"Augmented Var {i-1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bf71a",
   "metadata": {},
   "source": [
    "## Conclusion for Training Pipeline\n",
    "\n",
    "This strategy effectively \"breaks\" the perfect studio look of the dataset. By the time the model finishes training, it will have seen millions of variations in lighting, angle, and position, making it significantly more reliable for real-world application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
