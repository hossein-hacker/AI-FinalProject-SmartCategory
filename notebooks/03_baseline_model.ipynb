{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Model Training\n",
        "\n",
        "This notebook implements the baseline model for product category classification. The goal is to establish a simple, reproducible reference model to validate the dataset and provide a performance benchmark for future improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports\n",
        "\n",
        "First, we import the necessary libraries. We'll need `torch` for modeling and training, `pandas` for data handling, and `PIL` for image loading. We also import `transforms` from `torchvision` for basic image preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import hashlib\n",
        "import time\n",
        "from io import BytesIO\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Task and Inputs\n",
        "\n",
        "Here we define the core parameters for our training task.\n",
        "\n",
        "- **Number of Classes**: Based on our data preprocessing pipeline, we have a fixed number of final categories.\n",
        "- **Input Image Size**: We will resize all images to a standard size (e.g., 224x224) to ensure consistency.\n",
        "- **Output Format**: The model will output logits for each class, which will be passed through a `CrossEntropyLoss` function (which internally computes softmax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Dynamically determine the number of classes\n",
        "mapping_df = pd.read_csv('../data/processed/category_mapping.csv')\n",
        "NUM_CLASSES = mapping_df['merged_category_id'].nunique()\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 10\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# --- Paths ---\n",
        "DATA_PATH = '../data/processed/products_cleaned.csv'\n",
        "IMAGE_DIR = '../data/raw/images/' # Directory where images are stored\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "full_df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Original dataset size: {len(full_df)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset modification\n",
        "in order to speed up the baseline model setup and train, we filter out some categories and select some of the products in the remaining categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Identify categories with >= 25,000 samples\n",
        "cat_counts = full_df['merged_category_id'].value_counts()\n",
        "valid_cats = cat_counts[cat_counts >= 25000].index.tolist()\n",
        "print(f\"Found {len(valid_cats)} categories with >= 25,000 images.\")\n",
        "\n",
        "# 2. Filter the dataframe to only these categories\n",
        "full_df = full_df[full_df['merged_category_id'].isin(valid_cats)]\n",
        "\n",
        "# 3. Downsample each category to exactly 25,000\n",
        "# group_keys=False keeps the original index structure\n",
        "full_df = full_df.groupby('merged_category_id', group_keys=False).apply(lambda x: x.sample(25000, random_state=42))\n",
        "\n",
        "# 4. CRITICAL: Remap category IDs to contiguous 0...N-1 range\n",
        "# If we keep categories [0, 5, 10], the model will crash if we set NUM_CLASSES=3.\n",
        "# We must remap them to [0, 1, 2].\n",
        "unique_cats = sorted(full_df['merged_category_id'].unique())\n",
        "old_to_new_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_cats)}\n",
        "full_df['merged_category_id'] = full_df['merged_category_id'].map(old_to_new_mapping)\n",
        "\n",
        "# 5. Update global configuration\n",
        "# We overwrite the NUM_CLASSES from Step 2 to match this new filtered dataset\n",
        "NUM_CLASSES = len(unique_cats)\n",
        "\n",
        "print(f\"Filtered & Balanced Dataset: {len(full_df)} images ({NUM_CLASSES} classes x 25,000 images)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_utils import _url_to_filename\n",
        "\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "full_df['local_path'] = full_df['imgUrl'].fillna('').apply(\n",
        "    lambda u: os.path.join(IMAGE_DIR, _url_to_filename(u)) if isinstance(u, str) and u else ''\n",
        ")\n",
        "\n",
        "full_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare the Dataset for Training\n",
        "\n",
        "We define a custom PyTorch `Dataset` to load our data. It will read the `products_cleaned.csv` file, which contains image URLs and their corresponding category IDs. We will construct the local image path from the URL and the `IMAGE_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_utils import load_image_from_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing failed Image URLs\n",
        "There are some invalid URLs that we try to download them before the training. If still failed, we remove them from the dataset.\n",
        "<br>We do this before the dataset is divided into train/validate sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the failed URLs\n",
        "failed_urls_path = 'failed_urls.txt'\n",
        "try:\n",
        "    with open(failed_urls_path, 'r') as f:\n",
        "        failed_urls  = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    initial_count = len(full_df)\n",
        "    full_df = full_df[~full_df['imgUrl'].isin(failed_urls)]\n",
        "    final_count = len(full_df)\n",
        "\n",
        "    print(f\"\\nDataFrame Cleaned:\")\n",
        "    print(f\"- Removed {initial_count - final_count} broken rows.\")\n",
        "    print(f\"- Total valid samples: {final_count}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Failed URLs file {failed_urls_path} not found. No rows removed from dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductImageDataset(Dataset):\n",
        "    \"\"\"Custom dataset for loading product images from a CSV file or DataFrame.\"\"\"\n",
        "\n",
        "    def __init__(self, transform=None, df=None):\n",
        "        if df is not None:\n",
        "            self.df = df\n",
        "        else:\n",
        "            raise ValueError(\"'df' must be provided.\")\n",
        "        \n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        local_path = row['local_path']\n",
        "        \n",
        "        img = None\n",
        "        if local_path and os.path.exists(local_path):\n",
        "            try:\n",
        "                img = Image.open(local_path).convert(\"RGB\")\n",
        "            except Exception:\n",
        "                img = None\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Image is not cached! {row['title']}\")\n",
        "\n",
        "            # Try again loading the image\n",
        "            img = load_image_from_url(row['imgUrl'])\n",
        "        \n",
        "        # If image loading fails, return a dummy image\n",
        "        if img is None:\n",
        "            img = Image.new('RGB', IMAGE_SIZE, color = 'red')\n",
        "            \n",
        "        label = int(row['merged_category_id'])\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create DataLoaders\n",
        "\n",
        "Now we'll define our transformations and create `DataLoader` instances for training and validation. We'll also need to split our dataset. For a simple baseline, we'll do a basic 80/20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define separate transforms for training and validation\n",
        "# The training transform includes augmentation from the augmentation_steps notebook\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomRotation(degrees=20, fill=255),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=255),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# The validation transform is minimal (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Split the dataframe into training and validation sets, ensuring stratification\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['merged_category_id'])\n",
        "\n",
        "# Create separate datasets for training and validation with their respective transforms\n",
        "train_dataset = ProductImageDataset(df=train_df, transform=train_transform)\n",
        "val_dataset = ProductImageDataset(df=val_df, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "# num_workers = os.cpu_count()\n",
        "num_workers = 0\n",
        "print(\"Number of workers for DataLoader:\", num_workers)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=True)\n",
        "                        #   persistent_workers=(num_workers>0))\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=num_workers, pin_memory=True)\n",
        "                        # persistent_workers=(num_workers>0))\n",
        "\n",
        "print(f'Found {len(full_df)} total images.')\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Caching Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PRECACHE_STEP = True\n",
        "\n",
        "if PRECACHE_STEP:\n",
        "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "    import tqdm as _tqdm\n",
        "\n",
        "    tqdm = _tqdm.tqdm  # FORCE text-mode tqdm\n",
        "\n",
        "    def cache_image_only(url):\n",
        "        return load_image_from_url(url) is not None\n",
        "\n",
        "\n",
        "    MAX_WORKERS = 50\n",
        "    PREFETCH = 5000\n",
        "\n",
        "    urls = full_df['imgUrl'].dropna().unique()[::-1]\n",
        "    total = len(urls)\n",
        "\n",
        "    print(f\"Found {total} unique image URLs to download.\")\n",
        "    print(f\"Using {MAX_WORKERS} workers, prefetch={PREFETCH}\")\n",
        "\n",
        "    success = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = set()\n",
        "\n",
        "        pbar = tqdm(total=total, desc=\"Caching images\", unit=\"img\")\n",
        "\n",
        "        for url in urls:\n",
        "            futures.add(executor.submit(cache_image_only, url))\n",
        "\n",
        "            if len(futures) >= PREFETCH:\n",
        "                done = next(as_completed(futures))\n",
        "                futures.remove(done)\n",
        "\n",
        "                if done.result():\n",
        "                    success += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "        # drain remaining\n",
        "        for future in as_completed(futures):\n",
        "            if future.result():\n",
        "                success += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "    print(f\"--- Image caching complete! Downloaded {success} images. ---\")\n",
        "else:\n",
        "    print(\"Skipping precaching step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define the Baseline Model Architecture\n",
        "\n",
        "We use a simple Convolutional Neural Network (CNN) as our baseline. The architecture consists of three convolutional blocks, each followed by Batch Normalization, ReLU activation, and Max Pooling. A final adaptive average pooling layer and a linear classifier produce the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Define Training Configuration and Loop\n",
        "\n",
        "We use the Adam optimizer and Cross-Entropy Loss, which are standard choices for classification tasks. We then define a training function that iterates through the data for a specified number of epochs, computes the loss, and updates the model weights. We also include a validation function to evaluate the model's performance on the validation set after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in tqdm(loader, desc='Training'):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(loader, desc='Validation'):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return total_loss / len(loader), accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train the Model\n",
        "\n",
        "Now, we execute the training loop for the specified number of epochs, printing the training and validation loss and accuracy at each step. We store the results in a history dictionary for later visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'--- Epoch {epoch+1}/{NUM_EPOCHS} ---')\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_accuracy = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_accuracy'].append(val_accuracy)\n",
        "    \n",
        "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%')\n",
        "print('--- Training Complete ---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate and Interpret the Results\n",
        "\n",
        "Finally, we visualize the training and validation loss curves and the validation accuracy curve. This helps us understand the model's learning behavior and identify potential issues like overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Training Loss')\n",
        "ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "ax1.set_title('Loss Curves')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "*(This section should be filled in after running the notebook and observing the results.)*\n",
        "\n",
        "Based on the plots, we can conclude...\n",
        "\n",
        "**Weaknesses:**\n",
        "- ...\n",
        "\n",
        "**Hypotheses for Improvement (Phase-2):**\n",
        "- ...\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
