{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Model Training\n",
        "\n",
        "This notebook implements the baseline model for product category classification. The goal is to establish a simple, reproducible reference model to validate the dataset and provide a performance benchmark for future improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports\n",
        "\n",
        "First, we import the necessary libraries. We'll need `torch` for modeling and training, `pandas` for data handling, and `PIL` for image loading. We also import `transforms` from `torchvision` for basic image preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Task and Inputs\n",
        "\n",
        "Here we define the core parameters for our training task.\n",
        "\n",
        "- **Number of Classes**: Based on our data preprocessing pipeline, we have a fixed number of final categories.\n",
        "- **Input Image Size**: We will resize all images to a standard size (e.g., 224x224) to ensure consistency.\n",
        "- **Output Format**: The model will output logits for each class, which will be passed through a `CrossEntropyLoss` function (which internally computes softmax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Dynamically determine the number of classes\n",
        "mapping_df = pd.read_csv('data/processed/category_mapping.csv')\n",
        "NUM_CLASSES = mapping_df['merged_category_id'].nunique()\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 10\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# --- Paths ---\n",
        "DATA_PATH = 'data/processed/products_cleaned.csv'\n",
        "IMAGE_DIR = 'data/raw/images/' # Directory where images are stored\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "print(f'Found {NUM_CLASSES} classes.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare the Dataset for Training\n",
        "\n",
        "We define a custom PyTorch `Dataset` to load our data. It will read the `products_cleaned.csv` file, which contains image URLs and their corresponding category IDs. We will construct the local image path from the URL and the `IMAGE_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductImageDataset(Dataset):\n",
        "    \"\"\"Custom dataset for loading product images from a CSV file or DataFrame.\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, transform=None, csv_path=None, df=None):\n",
        "        if df is not None:\n",
        "            self.df = df\n",
        "        elif csv_path is not None:\n",
        "            self.df = pd.read_csv(csv_path)\n",
        "        else:\n",
        "            raise ValueError(\"Either 'df' or 'csv_path' must be provided.\")\n",
        "        \n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        # Construct the image path from the image URL\n",
        "        image_filename = os.path.basename(row['imgUrl'])\n",
        "        image_path = os.path.join(self.image_dir, image_filename)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            # Return a dummy red image if the file is not found\n",
        "            img = Image.new('RGB', IMAGE_SIZE, color = 'red')\n",
        "            \n",
        "        label = int(row['merged_category_id'])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create DataLoaders\n",
        "\n",
        "Now we'll define our transformations and create `DataLoader` instances for training and validation. We'll also need to split our dataset. For a simple baseline, we'll do a basic 80/20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define separate transforms for training and validation\n",
        "# The training transform includes augmentation from the augmentation_steps notebook\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomRotation(degrees=20, fill=255),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=255),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# The validation transform is minimal (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Split the dataframe into training and validation sets, ensuring stratification\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['merged_category_id'])\n",
        "\n",
        "# Create separate datasets for training and validation with their respective transforms\n",
        "train_dataset = ProductImageDataset(df=train_df, image_dir=IMAGE_DIR, transform=train_transform)\n",
        "val_dataset = ProductImageDataset(df=val_df, image_dir=IMAGE_DIR, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f'Found {len(full_df)} total images.')\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define the Baseline Model Architecture\n",
        "\n",
        "We use a simple Convolutional Neural Network (CNN) as our baseline. The architecture consists of three convolutional blocks, each followed by Batch Normalization, ReLU activation, and Max Pooling. A final adaptive average pooling layer and a linear classifier produce the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Define Training Configuration and Loop\n",
        "\n",
        "We use the Adam optimizer and Cross-Entropy Loss, which are standard choices for classification tasks. We then define a training function that iterates through the data for a specified number of epochs, computes the loss, and updates the model weights. We also include a validation function to evaluate the model's performance on the validation set after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in tqdm(loader, desc='Training'):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(loader, desc='Validation'):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return total_loss / len(loader), accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train the Model\n",
        "\n",
        "Now, we execute the training loop for the specified number of epochs, printing the training and validation loss and accuracy at each step. We store the results in a history dictionary for later visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'--- Epoch {epoch+1}/{NUM_EPOCHS} ---')\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_accuracy = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_accuracy'].append(val_accuracy)\n",
        "    \n",
        "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%')\n",
        "print('--- Training Complete ---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate and Interpret the Results\n",
        "\n",
        "Finally, we visualize the training and validation loss curves and the validation accuracy curve. This helps us understand the model's learning behavior and identify potential issues like overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Training Loss')\n",
        "ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "ax1.set_title('Loss Curves')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "*(This section should be filled in after running the notebook and observing the results.)*\n",
        "\n",
        "Based on the plots, we can conclude...\n",
        "\n",
        "**Weaknesses:**\n",
        "- ...\n",
        "\n",
        "**Hypotheses for Improvement (Phase-2):**\n",
        "- ...\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
